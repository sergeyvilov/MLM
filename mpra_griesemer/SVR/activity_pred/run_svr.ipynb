{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20d5284-43a5-4c57-9006-828f0883a2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import sklearn.pipeline \n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import optuna\n",
    "\n",
    "sys.path.append(\"/data/ouga/home/ag_gagneur/l_vilov/workspace/species-aware-DNA-LM/mpra_griesemer/utils\") \n",
    "\n",
    "from models import *\n",
    "from misc import dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e82215c-3d72-4985-acbf-fd521ff920d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/s/project/mll/sergey/effect_prediction/MLM/griesemer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62141475-833c-4454-9c27-137e8d7686f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_params = dotdict({})\n",
    "\n",
    "input_params.cell_type = 'HEK293FT' #HMEC,HEK293FT,HEPG2,K562,GM12878,SKNSH\n",
    "\n",
    "input_params.model = 'griesemer' #embedding name, can be \"MLM\" \"word2vec\" \"griesemer\" or \"Nmers\" where N is an integer\n",
    "\n",
    "input_params.output_dir = './test' #output folder\n",
    "\n",
    "input_params.N_trials = 1000 #number of optuna trials\n",
    "input_params.keep_first = True #perform hpp search only at the first split, then use these hyperparameters\n",
    "\n",
    "input_params.N_splits = 3 #number of GroupShuffleSplits\n",
    "input_params.N_CVsplits = 5 #number of CV splits for hyperparameter search\n",
    "input_params.seed = 1 #seed fot GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b876caad-6e6f-46b4-a7f4-a24323af5bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpra_df = pd.read_csv(data_dir + 'mpra_df.tsv', sep='\\t') #sequence info\n",
    "\n",
    "mlm_embeddings = np.load(data_dir + \"embeddings/seq_len_5000/embeddings.npy\") #masked language model embeddings\n",
    "\n",
    "#Data Cleaning\n",
    "# Take only SNP mutations\n",
    "# Remove nan values in Expression column\n",
    "\n",
    "is_snp = mpra_df.ref_allele.str.len() == mpra_df.alt_allele.str.len()\n",
    "\n",
    "flt = mpra_df[f'log2FoldChange_Skew_{input_params.cell_type}'].isna()  | (~is_snp) | (mpra_df.stop_codon_dist>5000) #| mpra_df.oligo_id.str.contains('_ref$')\n",
    "\n",
    "mpra_df = mpra_df[~flt]\n",
    "mlm_embeddings = mlm_embeddings[~flt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cceb1dd-9621-45a6-8e4a-170c7f3d4542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Expression column to float\n",
    "\n",
    "mpra_df['Expression'] = mpra_df.apply(lambda x: x[f'log2FoldChange_Alt_{input_params.cell_type}'] if x.oligo_id.endswith('_alt') else x[f'log2FoldChange_Ref_{input_params.cell_type}'], axis=1)   \n",
    "mpra_df.Expression = mpra_df.Expression.apply(lambda x:x.replace(',','.') if type(x)==str else x).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90ca9f16-a4a4-49fd-a1a5-6a32e1b43a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if input_params.model=='MLM':\n",
    "\n",
    "    X = mlm_embeddings\n",
    "\n",
    "elif 'mers' in input_params.model:\n",
    "    \n",
    "    k = int(input_params.model[0])\n",
    "        \n",
    "    kmerizer = Kmerizer(k=k)\n",
    "    X = np.stack(mpra_df.seq.apply(lambda x: kmerizer.kmerize(x))) \n",
    "        \n",
    "elif input_params.model=='word2vec':\n",
    "        \n",
    "    X = word2vec_model(mpra_df)\n",
    "\n",
    "elif input_params.model=='griesemer':\n",
    "        \n",
    "    X = minseq_model(mpra_df)\n",
    "\n",
    "X = np.hstack((X,np.expand_dims(mpra_df.min_free_energy.values,axis=1)))\n",
    "\n",
    "y = mpra_df['Expression'].values\n",
    "groups = mpra_df['group'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc5beaf-d2ee-45ad-a40f-309609edff82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hpp_search(X,y,groups,cv_splits = 5):\n",
    "    \n",
    "    '''\n",
    "    Perform Hyperparameter Search using OPTUNA Bayesian Optimisation strategy\n",
    "    \n",
    "    The bets hyperparameters should maximize coefficient of determination (R2)\n",
    "    \n",
    "    The hyperparameter range should first be adjused with grid search to make the BO algorithm converge in reasonable time\n",
    "    '''\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        C = trial.suggest_float(\"C\", 1e-2, 1e2, log=True)\n",
    "        epsilon = trial.suggest_float(\"epsilon\", 1e-5, 1, log=True)\n",
    "        gamma = trial.suggest_float(\"gamma\", 1e-5, 1, log=True)\n",
    "\n",
    "        clf = sklearn.svm.SVR(C=C, epsilon=epsilon, gamma=gamma)\n",
    "\n",
    "        pipe = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(),clf)\n",
    "\n",
    "        cv_score = sklearn.model_selection.cross_val_score(pipe, X, y, groups=groups, \n",
    "                     cv = sklearn.model_selection.GroupKFold(n_splits = cv_splits), scoring = 'r2', n_jobs = -1)\n",
    "        \n",
    "        av_score = cv_score.mean()\n",
    "        \n",
    "        return av_score\n",
    "    \n",
    "    study = optuna.create_study(direction = \"maximize\")\n",
    "\n",
    "    study.optimize(objective, n_trials = input_params.N_trials)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd7a3b-78ea-4d80-98d8-e5d366002ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running parallel\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVR\n",
      "\n",
      "\n",
      "predicting with SVR\n",
      "predicting with SVRpredicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "predicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "\n",
      "\n",
      "predicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "\n",
      "predicting with SVR\n",
      "predicting with SVR\n",
      "predicting with SVR\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "\n",
      "predicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "predicting with SVR\n",
      "predicting with SVR\n",
      "predicting with SVRpredicting with SVRpredicting with SVRpredicting with SVRpredicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVRpredicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVR\n",
      "\n",
      "\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVR\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "\n",
      "\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "\n",
      "\n",
      "predicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "\n",
      "\n",
      "\n",
      "predicting with SVR\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVRpredicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVRpredicting with SVRpredicting with SVR\n",
      "predicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVR\n",
      "predicting with SVRpredicting with SVR\n",
      "\n",
      "\n",
      "\n",
      "predicting with SVR\n",
      "predicting with SVR\n",
      "predicting with SVR\n",
      "\n",
      "predicting with SVR\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gss = sklearn.model_selection.LeaveOneGroupOut() \n",
    "\n",
    "train_idx, _ = next(iter(gss.split(X, y, groups)))\n",
    "\n",
    "best_hpp = {}\n",
    "\n",
    "#best_hpp = hpp_search(X[train_idx],y[train_idx],groups[train_idx],cv_splits = input_params.N_CVsplits) #get optimal hyperparameters\n",
    "\n",
    "#best_hpp = {'C': 0.03943153578419499, 'epsilon': 0.0712140417882623, 'gamma': 0.000232694021502066}\n",
    "\n",
    "def apply_regression(args):\n",
    "    \n",
    "    train_idx, test_idx = args\n",
    "\n",
    "    #predict with SVR\n",
    "    \n",
    "    print('predicting with SVR')\n",
    "    \n",
    "    pipe = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(),\n",
    "                                              sklearn.svm.SVR(**best_hpp))\n",
    "        \n",
    "    pipe.fit(X[train_idx],y[train_idx])  \n",
    "    \n",
    "    y_pred_svr = pipe.predict(X[test_idx])  \n",
    "    \n",
    "    #predict with Lasso\n",
    "    #use inner CV loop to adjust alpha\n",
    "    \n",
    "    group_kfold = sklearn.model_selection.GroupKFold(n_splits=input_params.N_CVsplits).split(X[train_idx],y[train_idx],groups[train_idx])\n",
    "    \n",
    "    pipe_lasso = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(), sklearn.linear_model.LassoCV(cv=group_kfold, alphas=10.**np.arange(-6,0))) \n",
    "    \n",
    "    pipe_lasso.fit(X[train_idx],y[train_idx])\n",
    "    \n",
    "    y_pred_lasso = pipe_lasso.predict(X[test_idx])\n",
    "        \n",
    "    print('done')\n",
    "\n",
    "    return list(zip(mpra_df.oligo_id.iloc[test_idx], y_pred_svr, y_pred_lasso))\n",
    " \n",
    "def run_pool():\n",
    "    \n",
    "    all_res = []\n",
    "    \n",
    "    pool = Pool(processes=input_params.n_jobs,maxtasksperchild=3)\n",
    "\n",
    "    for res in pool.imap(apply_regression,gss.split(X,y,groups)):\n",
    "        all_res.extend(res)\n",
    "     \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return all_res\n",
    "\n",
    "print('running parallel')\n",
    "\n",
    "all_res = run_pool()\n",
    "\n",
    "all_res = pd.DataFrame(all_res, columns=['oligo_id','y_pred_svr','y_pred_lasso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e89a5f-dece-4b51-83b7-3b3179771649",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpra_df.merge(all_res).to_csv(input_params.output_dir + f'/{input_params.cell_type}-{input_params.model}.tsv', sep='\\t', index=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03f0a5-2b36-4fad-a01f-cb2866eaa2a4",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd15461-1efb-4ad0-b8c5-f11efba5760a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cv_res = np.zeros((input_params.N_splits,len(y))) #predictions for each point in each split\n",
    "\n",
    "cv_res = np.zeros((len(set(groups)),len(y))) #predictions for each point in each split\n",
    "cv_res[:] = np.NaN \n",
    "\n",
    "cv_scores = [] #scores and best hyperparameters for each split\n",
    "\n",
    "gss = sklearn.model_selection.LeaveOneGroupOut() \n",
    "\n",
    "#gss = sklearn.model_selection.GroupShuffleSplit(n_splits=input_params.N_splits, train_size=.9, random_state = input_params.seed) \n",
    "\n",
    "for round_idx, (train_idx, test_idx) in enumerate(gss.split(X, y, groups)):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = X[train_idx,:],X[test_idx,:],y[train_idx],y[test_idx]\n",
    "        \n",
    "        if round_idx==0 or input_params.keep_first==False:\n",
    "            #perform only ones if input_params.keep_first==True\n",
    "            best_hpp = hpp_search(X_train,y_train,groups[train_idx],cv_splits = input_params.N_CVsplits)\n",
    "        \n",
    "        pipe = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(),\n",
    "                                              sklearn.svm.SVR(**best_hpp))\n",
    "        \n",
    "        pipe.fit(X_train,y_train)\n",
    "                    \n",
    "        y_pred = pipe.predict(X_test) \n",
    "        \n",
    "        cv_res[round_idx,test_idx] = y_pred\n",
    "        \n",
    "        cv_scores.append({'round':round_idx,\n",
    "                          'gene':groups[test_idx][0],\n",
    "                          'r2':sklearn.metrics.r2_score(y_test,y_pred)}|best_hpp)\n",
    "        \n",
    "cv_scores = pd.DataFrame(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33c8ba-5c3a-49b2-ac80-ee3598ad66d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(input_params.output_dir, exist_ok=True) #make output dir\n",
    "\n",
    "cv_scores.to_csv(input_params.output_dir + '/cv_scores.tsv', sep='\\t', index=None) #save scores\n",
    "\n",
    "with open(input_params.output_dir + '/cv_res.npy', 'wb') as f:\n",
    "    np.save(f, cv_res) #save predictions at each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b490b-5b1e-473c-82c8-7fb445c1cbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-svilov-mlm]",
   "language": "python",
   "name": "conda-env-miniconda3-svilov-mlm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
