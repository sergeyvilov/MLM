{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc9794c-c682-4f51-8295-36477b6c6c31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /s/project/mll/sergey/effect_prediction/MLM/baseline/dnabert/default/6-new-12w-0/ were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/s/project/mll/sergey/effect_prediction/MLM/baseline/dnabert/default/6-new-12w-0/\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"/s/project/mll/sergey/effect_prediction/MLM/baseline/dnabert/default/6-new-12w-0/\")\n",
    "\n",
    "\n",
    "import torch \n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "from collections.abc import Mapping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a16e7-d43d-4817-83e9-09842ada9262",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32485ee-d112-4d0b-8d02-34049891cdaf",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa321a1c-e4a8-4efb-8bd0-65588c985d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunkstring(string, length):\n",
    "    # chunks a string into segments of length\n",
    "    return (string[0+i:length+i] for i in range(0, len(string), length))\n",
    "\n",
    "def kmers(seq, k=6):\n",
    "    # splits a sequence into non-overlappnig k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq), k) if i + k <= len(seq)]\n",
    "\n",
    "def kmers_stride1(seq, k=6):\n",
    "    # splits a sequence into overlapping k-mers\n",
    "    return [seq[i:i + k] for i in range(0, len(seq)-k+1)]   \n",
    "\n",
    "def tok_func(x): return tokenizer(\" \".join(kmers_stride1(x[\"seq_chunked\"])))\n",
    "\n",
    "def one_hot_encode(gts, dim=5):\n",
    "    result = []\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    for nt in gts:\n",
    "        vec = np.zeros(dim)\n",
    "        vec[nuc_dict[nt]] = 1\n",
    "        result.append(vec)\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "def class_label_gts(gts):\n",
    "    nuc_dict = {\"A\":0,\"C\":1,\"G\":2,\"T\":3}\n",
    "    return np.array([nuc_dict[x] for x in gts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63eb49f-0ab2-4f0c-8557-f476c4485ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from transformers import  DataCollatorForLanguageModeling\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability = 0.15)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    length_of_first = examples[0].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "class DataCollatorForLanguageModelingSpan():\n",
    "    def __init__(self, tokenizer, mlm, mlm_probability, span_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm = mlm\n",
    "        self.span_length =span_length\n",
    "        self.mlm_probability= mlm_probability\n",
    "        self.pad_to_multiple_of = span_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        # If special token mask has been preprocessed, pop it from the dict.\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask):\n",
    "        import torch\n",
    "\n",
    "        labels = inputs.clone()\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.2)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        m_save = masked_indices.clone()\n",
    "        \n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability*0.8) \n",
    "        probability_matrix.masked_fill_(masked_indices, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool().numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * self.span_length, mode = 'same' ),axis = 1, arr = masked_indices).astype(bool) \n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        m_final = masked_indices + m_save \n",
    "        labels[~m_final] = -100  # We only compute loss on masked tokens\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        #indices_replaced = torch.bernoulli(torch.full(labels.shape, 1.0)).bool()\n",
    "        #print (indices_replaced)\n",
    "        inputs[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "        #print (masked_indices)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        #indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        #random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        #inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b1cbc-11ab-4355-acb8-41a92d826060",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d3c3a7-f9d8-4d48-aa48-19a1fca8c1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_batch(tokenized_data, dataset, seq_idx):\n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    label = dataset.iloc[seq_idx]['UTR3_seq']\n",
    "    label_len = len(label)\n",
    "    if label_len < 6:\n",
    "        return torch.zeros(label_len,label_len,5)\n",
    "    else:\n",
    "        diag_matrix = torch.eye(tokenized_data['input_ids'].shape[1]).numpy()\n",
    "        masked_indices = np.apply_along_axis(lambda m : np.convolve(m, [1] * 6, mode = 'same' ),axis = 1, arr = diag_matrix).astype(bool)\n",
    "        masked_indices = torch.from_numpy(masked_indices)\n",
    "        masked_indices = masked_indices[3:label_len-5-2]\n",
    "        res = tokenized_data['input_ids'].expand(masked_indices.shape[0],-1).clone()\n",
    "        res[masked_indices] = 4\n",
    "        #print (res[0], res.shape)\n",
    "        res = res.to(device)\n",
    "        with torch.no_grad():\n",
    "            fin_calculation = torch.softmax(model(res)['logits'], dim=2).detach().cpu()   \n",
    "        return fin_calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9fe45-2c83-445f-b990-1f0dc8f7efbb",
   "metadata": {},
   "source": [
    "## Translating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a93927f-c4c5-4f4f-8452-5cbfdb022e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_prbs_from_pred(prediction, pred_pos, token_pos, label_pos, label):   \n",
    "    # pred_pos = \"kmer\" position in tokenized sequence (incl. special tokens)\n",
    "    # token_pos = position of nucleotide in kmer\n",
    "    # label_pos = position of actual nucleotide in sequence\n",
    "    model_pred = prediction\n",
    "    prbs = [torch.sum(model_pred[pred_pos,tokendict_list[token_pos][nuc]]) for nuc in [\"A\",\"C\",\"G\",\"T\"]]\n",
    "    gt = label[label_pos] # 6-CLS, zerobased\n",
    "    res = torch.tensor(prbs+[0.0])\n",
    "    return res, gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca263894-392a-44f5-bb47-f0ec30eebbe9",
   "metadata": {},
   "source": [
    "# Prepare inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a469f-dc86-465a-81bc-3a7923631f3c",
   "metadata": {},
   "source": [
    "## Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5257c3d6-b7bb-4520-a13a-4c1ffc6c6037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv(\"/s/project/semi_supervised_multispecies/all_fungi_reference/fungi/Annotation/Sequences/AAA_Concatenated/Scer_half_life.csv\")\n",
    "#dataset = pd.read_csv(\"/s/project/semi_supervised_multispecies/Downstream/gPAR_CLIP/gpar_clip_downstream.csv\")\n",
    "dataset = pd.read_csv('/s/project/mll/sergey/effect_prediction/MLM/Gankin_data/gpar_clip_downstream.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd8c28c1-4ec4-4f6f-ade6-ff11c3db475f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_fasta = '/s/project/mll/sergey/effect_prediction/MLM/fasta/240_mammals/species/Homo_sapiens.fa'\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset = defaultdict(str)\n",
    "\n",
    "with open(human_fasta, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            seq_name = line[1:].split(':')[0]\n",
    "        else:\n",
    "            dataset[seq_name] += line.rstrip().upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d93af7b3-e99d-456a-89b9-3c505850a2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(list(dataset.items()), columns=['seq_name','UTR3_seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "599901ca-0b3a-44d8-9d9f-f380b702ed4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset['seq_len'] = dataset['UTR3_seq'].apply(lambda x: len(x))\n",
    "\n",
    "dataset['seq_chunked'] = dataset['UTR3_seq'].apply(lambda x : list(chunkstring(x, 510))) #chunk string in segments of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd40488a-d93e-4322-aa94-b7adb82b9b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = '/s/project/mll/sergey/effect_prediction/MLM/baseline/dnabert/default/predictions/'\n",
    "\n",
    "dataset_start = 0\n",
    "dataset_len = 1500\n",
    "\n",
    "dataset = dataset.iloc[dataset_start:dataset_start+dataset_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2319883a-446c-4d6b-ac3e-ac87d87a5de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.explode('seq_chunked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "230e0d37-9326-4db7-8a16-76afc6247be9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16cdebb49f44d68a858b606af223c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(dataset[['seq_chunked']])\n",
    "\n",
    "tok_ds = ds.map(tok_func, batched=False,  num_proc=2)\n",
    "\n",
    "rem_tok_ds = tok_ds.remove_columns('seq_chunked')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModelingSpan(tokenizer, mlm=False, mlm_probability = 0.025, span_length =6)\n",
    "data_loader = torch.utils.data.DataLoader(rem_tok_ds, batch_size=1, collate_fn=data_collator, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed132ad6-0aad-4cab-8dde-919719c02abc",
   "metadata": {},
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04f98016-880a-43c7-993c-35fe8fd5b69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/transformers/modeling_utils.py:1900\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1898\u001b[0m     )\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 593\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/cuda/__init__.py:214\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    218\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44910007-d73d-4499-83b3-2fec77d686be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m computed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/transformers/modeling_utils.py:1900\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1898\u001b[0m     )\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 593\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/nn/modules/module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/svilov-mlm/lib/python3.9/site-packages/torch/cuda/__init__.py:214\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    218\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "computed = []\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd982373-8c17-46c0-abed-7e1c72df1eed",
   "metadata": {},
   "source": [
    "## Prepare tokendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a615b9f0-185f-4de2-a6fe-910cc6c992be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokendict_list = [{\"A\": [], \"G\": [], \"T\": [],\"C\": []} for x in range(6)]\n",
    "\n",
    "for tpl in itertools.product(\"ACGT\",repeat=6):\n",
    "    encoding = tokenizer.encode(\"\".join(tpl))\n",
    "    for idx, nuc in enumerate(tpl):\n",
    "        tokendict_list[idx][nuc].append(encoding[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a7a66-b2bf-4525-b2e5-ecff6ae664f1",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f00660-95e0-4d15-b7d7-54c349eab162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k = 6\n",
    "predicted_prbs,gts = [],[]\n",
    "#print (dataset.iloc[0]['seq_chunked'])\n",
    "\n",
    "for no_of_index, tokenized_data in tqdm.tqdm(enumerate(data_loader)):\n",
    "    #if no_of_index < 1340:\n",
    "    #    continue\n",
    "    label = dataset.iloc[no_of_index]['seq_chunked']\n",
    "    label_len = len(label)\n",
    "    #print(no_of_index, label_len)\n",
    "    \n",
    "    # Edge case: for a sequence less then 11 nt\n",
    "    # we cannot even feed 6 mask tokens\n",
    "    # so we might as well predict random\n",
    "    if label_len < 11: \n",
    "        #print (no_of_index)\n",
    "        for i in range(label_len):\n",
    "            predicted_prbs.append(torch.tensor([0.25,0.25,0.25,0.25,0.0]))\n",
    "            gts.append(label[i])\n",
    "        continue\n",
    "\n",
    "        \n",
    "    model_input_unaltered = tokenized_data['input_ids'].clone()\n",
    "    tokenized_data['labels'][tokenized_data['labels']==-100] = 0\n",
    "    inputs = model_input_unaltered.clone()\n",
    "    \n",
    "\n",
    "    # First 5 nucleotides we infer from the first 6-mer\n",
    "    inputs[:, 1:7] = 4 # we mask the first 6 6-mers\n",
    "    inputs = inputs.to(device) \n",
    "    model_pred = torch.softmax(model(inputs)['logits'], dim=2)\n",
    "    \n",
    "    for i in range(5):\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred[0],\n",
    "                                        pred_pos=1, # first 6-mer (after CLS)\n",
    "                                        token_pos=i, # we go thorugh first 6-mer\n",
    "                                        label_pos=i,\n",
    "                                        label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "    \n",
    "    \n",
    "\n",
    "    # we do a batched predict to process the rest of the sequence\n",
    "    predictions = predict_on_batch(tokenized_data, dataset, no_of_index)\n",
    "    \n",
    "    # For the 6th nt up to the last 5 \n",
    "    # we extract probabilities similar to how the model was trained\n",
    "    # hiding the 4th nt of the 3rd masked 6-mer of a span of 6 masked 6-mers\n",
    "    # note that CLS makes the tokenized seq one-based\n",
    "    pos = 5 # position in sequence\n",
    "    for pos in range(5, label_len-5):\n",
    "        model_pred = predictions[pos-5]\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred,\n",
    "                                        pred_pos=pos-2, # for i-th nt, we look at (i-2)th 6-mer\n",
    "                                        token_pos=3, # look at 4th nt in 6-mer\n",
    "                                        label_pos=pos,\n",
    "                                        label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "        \n",
    "    # Infer the last 5 nt from the last 6-mer\n",
    "    for i in range(5):\n",
    "        model_pred = predictions[pos-5]\n",
    "        res,gt = extract_prbs_from_pred(prediction=model_pred,\n",
    "                                pred_pos=pos+1, # len - 5 + 1 = last 6-mer (1-based)\n",
    "                                token_pos=i+1, # we go through last 5 of last 6-mer\n",
    "                                label_pos=pos+i,\n",
    "                                label=label)\n",
    "        predicted_prbs.append(res)\n",
    "        gts.append(gt)\n",
    "\n",
    "    assert(len(gts) == torch.stack(predicted_prbs).shape[0]), \"{} iter, expected len:{} vs actual len:{}\".format(no_of_index,\n",
    "                                                                                   len(gts), \n",
    "                                                                                   torch.stack(predicted_prbs).shape[0])\n",
    "\n",
    "    #XABCDEFGHIJKL -> XABCDE [ABCDEF BCDEFG CDEFGH DEFGHI EFGHIJ FGHIJK] GHIJKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "703744b9-8854-459b-b746-a1f907a14c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[['seq_name','seq_chunked']].to_csv(output_dir + f\"seq_{dataset_start}.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ebe5ac0c-e521-46b6-ad38-a8915a810788",
   "metadata": {},
   "outputs": [],
   "source": [
    "prbs_arr = np.array(torch.stack(predicted_prbs))\n",
    "np.save(output_dir + f\"preds_{dataset_start}.npy\", prbs_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564eb1b9-1249-464a-ad28-4de86b1feb43",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "24320f1c-c55c-4a64-942c-aa9f24307cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49436766"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.max(prbs_arr,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d896b80c-0362-460e-9cfc-013689861d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3954733649623095"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "np.sum(gts == np.array([\"A\",\"C\",\"G\",\"T\"])[np.argmax(prbs_arr,axis=1)])/len(gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "25411478-cf13-4ecc-bcaf-bc9591af8094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Actual 0.33590633376540024, Predicted 0.388372575915835\n",
      "C: Actual 0.15707153014610573, Predicted 0.07040855492493361\n",
      "G: Actual 0.14310905704541738, Predicted 0.04891584762719756\n",
      "T: Actual 0.3639130790430766, Predicted 0.49230302153203376\n"
     ]
    }
   ],
   "source": [
    "for nt in [\"A\", \"C\", \"G\", \"T\"]:\n",
    "    nt_arr = np.array([nt]*len(gts))\n",
    "    actual = np.sum(gts == nt_arr)/len(gts)\n",
    "    predicted = np.sum(np.array([\"A\",\"C\",\"G\",\"T\"])[np.argmax(prbs_arr,axis=1)] == nt_arr)/len(gts)\n",
    "    print(\"{}: Actual {}, Predicted {}\".format(nt, actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a42eec2b-ec52-4142-86f2-025b4387953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prbs = torch.log(torch.stack(predicted_prbs)[:,:-1])\n",
    "class_labels = torch.tensor(class_label_gts(gts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fdc7e2e8-b962-4f14-82d4-4d512a7b11cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4430)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.nll_loss(log_prbs, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2aa84-aa6d-418c-beeb-d64fcbdbe56e",
   "metadata": {},
   "source": [
    "# Make data fit metrics handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "aa217b54-a115-4c44-bd8b-af28858fd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_path = \"outputs/gpar_bertadn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821776e-d73e-48fe-9517-a70d8fc86b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get targets\n",
    "targets = torch.tensor(class_label_gts(gts))\n",
    "stacked_prbs = torch.stack(predicted_prbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5b1ac2f0-f18b-42cd-8ef3-a0f145bd32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cross entropy, it's already as probability so just nll\n",
    "ce = torch.nn.functional.nll_loss(stacked_prbs, targets, reduction=\"none\") #cross_entropy(prbs, targets)\n",
    "\n",
    "#print(ce)\n",
    "\n",
    "# save\n",
    "torch.save(stacked_prbs,  out_path+\"masked_logits.pt\") # no logits, so use prbs\n",
    "torch.save(torch.argmax(stacked_prbs, dim=1),  out_path+\"masked_preds.pt\")\n",
    "torch.save(stacked_prbs,  out_path+\"prbs.pt\")\n",
    "torch.save(ce, out_path+\"ce.pt\")\n",
    "\n",
    "# save targets\n",
    "torch.save(targets, out_path+\"masked_targets.pt\")\n",
    "\n",
    "# save rest as placeholders (zeros of same length)\n",
    "torch.save(torch.zeros(len(stacked_prbs)), out_path+\"masked_motifs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6acad-bbbd-47c1-81bd-4fd9a5e5b7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-svilov-mlm]",
   "language": "python",
   "name": "conda-env-miniconda3-svilov-mlm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
