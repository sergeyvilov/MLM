{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c20d5284-43a5-4c57-9006-828f0883a2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import sklearn.pipeline \n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.linear_model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import optuna\n",
    "\n",
    "sys.path.append(\"/data/ouga/home/ag_gagneur/l_vilov/workspace/species-aware-DNA-LM/mpra_griesemer/utils\") \n",
    "\n",
    "from models import *\n",
    "from misc import dotdict\n",
    "\n",
    "import scipy.stats\n",
    "import pickle\n",
    "\n",
    "import gensim.models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e82215c-3d72-4985-acbf-fd521ff920d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/s/project/mll/sergey/effect_prediction/MLM/agarwal_2022/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62141475-833c-4454-9c27-137e8d7686f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_params = dotdict({})\n",
    "\n",
    "input_params.model = 'MLM' #embedding name, can be \"MLM\" \"word2vec\" \"griesemer\" or \"Nmers\" where N is an integer\n",
    "\n",
    "input_params.output_dir = './test' #output folder\n",
    "\n",
    "input_params.N_trials = 1000 #number of optuna trials\n",
    "input_params.keep_first = True #perform hpp search only at the first split, then use these hyperparameters\n",
    "\n",
    "input_params.N_splits = 100 #number of GroupShuffleSplits\n",
    "input_params.N_CVsplits = 5 #number of CV splits for hyperparameter search\n",
    "input_params.seed = 1 #seed fot GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89e4b90f-d162-4f9f-9045-a25f6d32670a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get MLM embeddings\n",
    "\n",
    "with open(data_dir + '../species_aware_emb/all_3utr.pickle', 'rb') as f:\n",
    "            mlm_embeddings = pickle.load(f)\n",
    "            utr_names =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "151a30e2-b600-4a47-9557-3f1ed3625ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#MLM embeddings are made for transcripts\n",
    "#get corresponding gene names\n",
    "\n",
    "embedding_transcripts = [x.split('.')[0] for x in utr_names]\n",
    "\n",
    "transcript_to_gene = pd.read_csv(data_dir + '../UTR_coords/GRCh38_EnsembleCanonical_HGNC.tsv.gz', sep='\\t', \n",
    "                                     names=['gene_id','transcript_id'], skiprows=1,usecols=[0,1]).set_index('transcript_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "102b76cb-4f0f-4056-87eb-65d119529c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get FASTA seqs\n",
    "\n",
    "human_fasta = data_dir + '../fasta/240_mammals/species/Homo_sapiens.fa'\n",
    "\n",
    "human_utr = defaultdict(str)\n",
    "\n",
    "with open(human_fasta, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('>'):\n",
    "            transcript_id = line[1:].split(':')[0].split('.')[0]\n",
    "        else:\n",
    "            human_utr[transcript_id] += line.rstrip().upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7589c703-e38c-4d11-bfec-bc08fbb93361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utr_df = pd.DataFrame(human_utr.values(),\n",
    "             index=transcript_to_gene.loc[human_utr.keys()].gene_id, \n",
    "             columns=['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "592968d4-5af7-4452-b0e1-133461abd573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds_df = pd.read_csv(data_dir + 'saluki_paper/Fig3_S4/binnedgenes.txt', sep='\\t', usecols=[0,1],\n",
    "                      names=['Fold','gene_id'], skiprows=1).set_index('gene_id') #folds as they are in Agarwal article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cfe7d849-8a98-4e80-be9a-1af814f9bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = [folds_df]\n",
    "\n",
    "df = pd.read_csv(data_dir + 'human/seqFeatWithKmerFreqs.txt.gz', sep='\\t', \n",
    "                          usecols=lambda x: not 'ORF.' in x and not 'UTR.' in x).set_index('GENE') #basic features (8) + codons (62)\n",
    "\n",
    "data_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e1babe65-eebf-4b44-80ff-9848eb8659d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SeqWeaver RBP binding (780)\n",
    "for region in ('3pUTR','5pUTR','ORF'):\n",
    "    df = pd.read_csv(data_dir + f'human/SeqWeaver_predictions/{region}_avg.txt.gz', sep='\\t').set_index('Group.1')\n",
    "    data_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "02d530db-2afb-473f-a1a1-f1c9a8503333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#miRNA target repression (319)\n",
    "df = pd.read_csv(data_dir + f'human/CWCS.txt.gz', sep='\\t').set_index('GeneID')\n",
    "data_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9aa53c7c-5eaf-4749-8afd-a4da0a73a56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_df = pd.concat(data_df,axis=1) #concat all features, except embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ac4ac09a-ff0b-4bce-bb1c-da784565d88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_df = data_df[~data_df.HALFLIFE.isna()]\n",
    "data_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "90ca9f16-a4a4-49fd-a1a5-6a32e1b43a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get sequence embedding depending on the model\n",
    "\n",
    "if input_params.model=='MLM':\n",
    "            \n",
    "\n",
    "    embeddings_df = pd.DataFrame(mlm_embeddings, \n",
    "                                     index=transcript_to_gene.loc[embedding_transcripts].gene_id, \n",
    "                                     columns=[f'emb_{x}' for x in range(mlm_embeddings.shape[1])])\n",
    "\n",
    "elif 'mers' in input_params.model:\n",
    "    \n",
    "    k = int(input_params.model[0])\n",
    "        \n",
    "    kmerizer = Kmerizer(k=k)\n",
    "    \n",
    "    Nmer_embeddings = utr_df.seq.apply(lambda x: kmerizer.kmerize(x))\n",
    "    \n",
    "    embeddings_df = pd.DataFrame(Nmer_embeddings.tolist(), index=Nmer_embeddings.index, columns=[f'emb_{x}' for x in range(4**k)])\n",
    "\n",
    "elif input_params.model=='word2vec':\n",
    "        \n",
    "    kmerizer_w2v = Kmerizer(k=4)\n",
    "\n",
    "    w2v_model = gensim.models.Word2Vec(sentences=utr_df.seq.apply(lambda x: kmerizer_w2v.tokenize(x)), \n",
    "                             vector_size=128, window=5, min_count=1, workers=4, sg=1) #default: CBOW\n",
    "\n",
    "    word2vec_emb = utr_df.seq.apply(\n",
    "        lambda x: np.mean([w2v_model.wv[x]  for x in kmerizer_w2v.tokenize(x)],axis=0)) #average embedding of all 4-mers in the sequence\n",
    "\n",
    "    word2vec_emb = word2vec_emb[~word2vec_emb.isna()]\n",
    "    \n",
    "    embeddings_df = pd.DataFrame(word2vec_emb.tolist(), index=word2vec_emb.index, columns=[f'emb_{x}' for x in range(128)])\n",
    "    \n",
    "data_df = pd.concat([data_df,embeddings_df], join='inner', axis=1)\n",
    "\n",
    "X = data_df.iloc[:,2:].values #all columns except HALFLIFE and fold\n",
    "y = data_df['HALFLIFE'].values\n",
    "folds = data_df['Fold'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5beaf-d2ee-45ad-a40f-309609edff82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_SVR(args):\n",
    "        \n",
    "    fold_idx, test_hpp = args \n",
    "\n",
    "    test_idx = mpra_df[mpra_df.Fold==str(fold_idx)].index\n",
    "    train_idx = mpra_df[(mpra_df.Fold!=str(fold_idx))&(mpra_df.Fold!='Test')].index\n",
    "\n",
    "    pipe = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(),\n",
    "                                                  sklearn.svm.SVR(**test_hpp))\n",
    "    pipe.fit(X[train_idx],y[train_idx])\n",
    "\n",
    "    R2_score = pipe.score(X[test_idx],y[test_idx])\n",
    "        \n",
    "    return R2_score\n",
    "\n",
    "\n",
    "def hpp_search(X,y, mpra_df, cv_splits = 10):\n",
    "    \n",
    "    '''\n",
    "    Perform Hyperparameter Search using OPTUNA Bayesian Optimisation strategy\n",
    "    \n",
    "    The bets hyperparameters should maximize coefficient of determination (R2)\n",
    "    \n",
    "    The hyperparameter range should first be adjused with grid search to make the BO algorithm converge in reasonable time\n",
    "    '''\n",
    "\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        C = trial.suggest_float(\"C\", 1e-4, 1e4, log=True)\n",
    "        epsilon = trial.suggest_float(\"epsilon\", 1e-5, 1, log=True)\n",
    "        gamma = trial.suggest_float(\"gamma\", 1e-5, 1, log=True)\n",
    "\n",
    "        test_hpp = {'C':C, 'epsilon':epsilon, 'gamma':gamma}\n",
    "        \n",
    "        pool = Pool(processes=input_params.n_jobs,maxtasksperchild=5)\n",
    "\n",
    "        cv_scores = []\n",
    "        \n",
    "        params = ((fold_idx, test_hpp) for fold_idx in range(cv_splits))\n",
    "        \n",
    "        for res in pool.imap(apply_SVR,params):\n",
    "            cv_scores.append(res)\n",
    "     \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    \n",
    "        return np.mean(cv_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction = \"maximize\")\n",
    "\n",
    "    study.optimize(objective, n_trials = input_params.N_trials)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd15461-1efb-4ad0-b8c5-f11efba5760a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cv_res = np.zeros((input_params.N_splits,len(y))) #predictions for each point in each split\n",
    "cv_res = np.zeros((len(set(groups)),len(y))) #predictions for each point in each split\n",
    "cv_res[:] = np.NaN \n",
    "\n",
    "#cv_res_lasso = np.zeros((input_params.N_splits,len(y))) #predictions for each point in each split\n",
    "cv_res_lasso = np.zeros((len(set(groups)),len(y))) #predictions for each point in each split\n",
    "cv_res_lasso[:] = np.NaN \n",
    "\n",
    "cv_scores = [] #scores and best hyperparameters for each split\n",
    "\n",
    "#gss = sklearn.model_selection.LeaveOneGroupOut() \n",
    "\n",
    "input_params.N_splits=10\n",
    "\n",
    "gss = sklearn.model_selection.LeaveOneGroupOut() \n",
    "\n",
    "best_hpp = {'C': 0.12227907014412719, 'epsilon': 0.002017028863020407, 'gamma': 0.0017418739573905068}#MLM\n",
    "best_hpp = {'C': 0.06074046127798098, 'epsilon': 0.032758360822165884, 'gamma': 0.0005466435051403621} #5mers\n",
    "\n",
    "#best_hpp = {'alpha':1e-5}\n",
    "\n",
    "for round_idx, (train_idx, test_idx) in enumerate(gss.split(X, y, groups)):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = X[train_idx,:],X[test_idx,:],y[train_idx],y[test_idx]\n",
    "        \n",
    "        if round_idx==0 or input_params.keep_first==False:\n",
    "            #perform only ones if input_params.keep_first==True\n",
    "            best_hpp = hpp_search(X_train,y_train,groups[train_idx],cv_splits = input_params.N_CVsplits)\n",
    "        \n",
    "        pipe = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(),\n",
    "                                              sklearn.svm.SVR(**best_hpp))\n",
    "        \n",
    "        #pipe = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(),\n",
    "        #                                      sklearn.linear_model.Lasso(**best_hpp))\n",
    "        \n",
    "        pipe.fit(X_train,y_train)\n",
    "                    \n",
    "        y_pred = pipe.predict(X_test) \n",
    "        \n",
    "        cv_res[round_idx,test_idx] = y_pred\n",
    "        \n",
    "        group_kfold = sklearn.model_selection.GroupKFold(n_splits=input_params.N_CVsplits).split(X_train,y_train,groups[train_idx])\n",
    "        pipe_lasso = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(), sklearn.linear_model.LassoCV(cv=group_kfold, alphas=10.**np.arange(-6,0))) \n",
    "        pipe_lasso.fit(X_train,y_train)\n",
    "        y_pred_lasso = pipe_lasso.predict(X_test)\n",
    "        \n",
    "        cv_res_lasso[round_idx,test_idx] = y_pred_lasso\n",
    "\n",
    "        cv_scores.append({'round':round_idx,\n",
    "                          'chrom':groups[test_idx][0],\n",
    "                         'r2_svr':sklearn.metrics.r2_score(y_test,y_pred),\n",
    "                         'pearson_r_svr':scipy.stats.pearsonr(y_test,y_pred)[0],\n",
    "                         'r2_lasso':sklearn.metrics.r2_score(y_test,y_pred_lasso),\n",
    "                         'pearson_r_lasso':scipy.stats.pearsonr(y_test,y_pred_lasso)[0]\n",
    "                         }|best_hpp)\n",
    "        \n",
    "cv_scores = pd.DataFrame(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "0b29d75f-44b1-4f67-a00d-4c8ada975935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2038695562423924"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores.pearson_r_svr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "026607b1-b03d-4935-bcd7-7c6b75dd7dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "f9e63190-f4b8-4130-ba2f-47fe4d22a84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pearson_r(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    #print(estimator[1].alpha_)\n",
    "    if len(y_pred.shape) == 2:\n",
    "        y_pred = y_pred.reshape(-1)\n",
    "    return scipy.stats.pearsonr(y, y_pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b661bd4-8fcf-44e3-b258-441b63151f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = sklearn.pipeline.make_pipeline(sklearn.preprocessing.StandardScaler(), sklearn.linear_model.LassoCV(cv=3, alphas=10.**np.arange(-6,0))) \n",
    "#limit runtime\n",
    "r2 = cross_val_score(pipe,X[train_idx],y[train_idx],scoring=pearson_r,\n",
    "                     cv=sklearn.model_selection.GroupKFold(n_splits=10), groups=groups[train_idx], n_jobs=-1)#.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "a414b297-c570-423a-8d94-a28d07570ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.496721636952289"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33c8ba-5c3a-49b2-ac80-ee3598ad66d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(input_params.output_dir, exist_ok=True) #make output dir\n",
    "\n",
    "cv_scores.to_csv(input_params.output_dir + '/cv_scores.tsv', sep='\\t', index=None) #save scores\n",
    "\n",
    "with open(input_params.output_dir + '/cv_res.npy', 'wb') as f:\n",
    "    np.save(f, cv_res) #save predictions at each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b490b-5b1e-473c-82c8-7fb445c1cbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-svilov-mlm]",
   "language": "python",
   "name": "conda-env-miniconda3-svilov-mlm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
